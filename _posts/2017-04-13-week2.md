---
layout: post
title: Week2 Study Notes
---

>本学期开始将在这里对每周所看文章、所思所想进行系统而又细致的梳理。每周从以下方面进行梳理

- 所看文章及梳理
- 所看代码
- 所编代码
- 自己的算法流程和主要参考文章

> 以上内容中，文章会通过链接的方式呈现出来，并会提供一个python脚本[暂无]进行批量下载，代码一般会给出git的仓库链接。
> 本周工作在上周工作的基础上，主要研究使用**增强学习**（RL）进行Image Caption的两篇文章（会在后面列出）。

----------------------------------

## 1.0 Papers List

>主要看的几篇文章或书中章节

**0** Richard S. Sutton and Andrew G. Barto. "**Reinforcement Learning: An Introduction**". Chapter 13 Policy Gradient Methods. [[link]](http://www.freetechbooks.com/reinforcement-learning-an-introduction-second-edition-draft-t1282.html)

**1** Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jarret Ross, Vaibhava Goel. "**Self-critical Sequence Training for Image Captioning**". [[link]](https://arxiv.org/abs/1612.00563)

**2** Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, Kevin Murphy. "**Optimization of image description metrics using policy gradient methods**". [[link]](https://arxiv.org/abs/1612.00370)

## 1.1 Other papers

>参考内容

**3** 知乎. "**深度增强学习之Policy Gradient方法1**". [[link]](https://zhuanlan.zhihu.com/p/21725498?refer=intelligentunit)

## 1.2 Notes

### 1.2.1 Policy Gradient

由于这篇论文[1][2]是通过增强学习中的策略梯度算法Policy Gradient(PG)来对现有的Image Caption的方法进行优化的，因此我先补习了一下增强学习方面尤其是有关于PG方面的知识。主要参考资料为**Reinforcement Learning: An Introduction**[0]这本书以及在网上找的一些资料([[David Silver的增强学习课程]](http://icml.cc/2016/tutorials/deep_rl_tutorial.pdf)、[[深度增强学习之Policy Gradient方法1]](https://zhuanlan.zhihu.com/p/21725498?refer=intelligentunit))。

在讲PG之前，我们先梳理一下增强学习的一些内容。

- Introduction to Reinforement Learning
- Value-Based Deep RL
- Policy-Based Deep RL

首先从增强学习（Reinforement Learning, RL）的开始。RL是机器学习中的一个领域，强调agent如何根据当前环境、所处状态行动，以得到最大化的预期估计。这里“增强”或者“强化”的意思是，根据不断地试错而得到的环境反馈来不断增强自己的决策。**agent**是个体，也是行动的主体，可以设想为机器人，它可以感知环境从而得到自己所处状态**state**，也可以执行行动**act**，当执行行动时会改变当前所处状态并且通过环境得到执行这一过程的反馈**reword**，而策略**policy**是根据当前的状态执行怎样的行动。增强学习的目标是选择让agent通过当前的状态来选择一个期望奖励最大的行动。

具体地，对于一个agent来说，它包含以下几个方面

- 策略：Policy，agent的行为函数，从状态到行为的映射，且分为两种：
 - 确定性策略，Deterministic policy
 - 随机策略，Stochastic policy
- 值函数，Value Function，评判状态或/和动作的好坏
-  模型，Model，如何表示环境

其中值函数可以认为是对未来奖惩的预测，就是说在当前这个状态s执行动作a之后能得到怎样的反馈，通常使用**Q值函数**来给出期望估计，Q值函数定义如下

$$ Q^{\pi}(s,a)=\mathbb{E}[r_{t+1}+\gamma r_{t+2}+\gamma^{2} r_{t+3}+\cdots|s,a] $$

上面的式子可以写成递归的Bellman等式，因此Q值函数可以通过递归的方法不断更新。我们要做的就是找到那个使Q值函数最大的策略$\pi^{*}$，最为直接的想法的根据前面的Bellman等式向前搜索，从而对于每个状态s执行每个动作a，给出一个价值value=q(s,a)，我们可以通过这个value制定相应的策略，如选择value最大的那个动作执行等。这就是所谓的Q Learning。

但是在实际操作过程中，对于复杂的任务以及环境而言，Q值函数是很难穷举完的。这时候该这怎么办呢？这时候需要对Q值函数进行模拟，大体思路是：建立一个Q值网络，然后agent通过不断地进行各种动作得到reword并对网络进行更新，最终得到一个可以准确地估计Q值函数的网络，**Deep Q Learning(DQN)**就是这么做的，只不过DQN利用了深度学习的一些网络。

与Q Learning不同的是，我们还有一种更加end-to-end的办法，即Policy Gradient。和 Deep Q Learning 用Q网络去估计Q 表然后在规定一种策略去依据Q值采取行动不同，Policy Gradient直的策略网络直接输出的就是策略，比如采取每一种行动的概率（对于离散控制问题），或者每一个动作的值（对于连续控制问题）。

PG的大体思路是这样的，我们构建一个策略网络\pi，其参数为\theta，制定一个关于\theta目标函数J(\theta)——获得reward的期望$J(\theta)=E_{\pi(\theta)[r]}$，我们可以求得J关于\theta的导数。*还没有细看求解过程，看了几篇论文中都有有关这部分的内容，需细看*。有了这个导数，我们就可以通过梯度下降算法来不断更新策略网络的参数\theta。将Monte Carlo算法与PG结合起来是为REINFORCE算法，*书中[0, chatper 13.3]有详细介绍还未细看*。

PG相对于Q Learning有两个显著的优点:
- End-to-End直接生成策略；
- 可以处理动作空间连续时的情景，而Q Learning不行。

另外，书中还介绍了一种优化PG的方法——baseline[0. chapter 13.4]，引入一个与a无关的比较函数baseline（这样可以保证求得的J关于\theta的导数不发生改变），两者作差，这样可以减少求得的累积reward的方差。*关于这一点有什么好处，暂时还没看明白。*在文章[1]中就是在baseline上作了文章。

*由于排版缘故以及时间原因，具体的数学上的形式化语言先在此省略。*

### 1.2.2 Optimization of image description metrics using policy gradient methods

这篇文章是看得时间比较多的文章，从文章题目可以看出来，作者是通过PG来对现有的Image Caption的metric进行优化，而这些metric也就是对生成结果的度量，所以可以看做是增强学习中的reward。

#### Abstract

>利用PG直接为metrics优化

文章推出了一个新的基于PG的Image Caption模型训练过程，可以让我们不用进行最大似然估计而直接为metric进行优化。metrics包括BLEU，CIDEr，METEOR和ROUGE（BCMR），其实这些衡量标准是如何得到的我还不清楚，还需看一下。另外最近新出了一个衡量标准，SPICE，文章对此也做了实验。

#### Introduction

这篇文章的落脚点在于对现有的最好Image Caption模型进行优化，所选模型为encoder-decoder模型，在这个模型中通过卷积神经网络对图片内容进行编码，在生成文本的时候通过RNN每次基于已经生成的单词生成下一个单词。

现有的方法的有个缺点——它们使用了最大似然估计（MLE）来进行训练。然而这种方法有两个比较大的困难：
- exposure bias，在训练的时候每次的输入都是来自真实的caption，而在测试的时候，生成的每一个单词都是基于之前生成的单词，一旦有一个单词生成的不好，后面生成的单词会积累前面生成单词的误差从而跑偏；
- MLE只会估算出被分配给真实句子的概率有多大，而忽略了所有生成的其他句子的质量。

解决第一个问题的办法是在训练的时候和评估的时候的一样，都用生成的句子，即“scheduled sampling”方法，但是这种方法在统计上是不一致的，虽然确实能在实践中提升性能。

更好的办法是，我们在训练的时候优化和评估的时候一样的metrics，如BCMR。BCMR有个问题是不可导，我们可以使用PG来优化它们，具体做法是**将候选句子的分数作为强化学习中的reowerd信号，将RNN译码器作为随机策略，生成一个单词就是执行一次动作**。在这篇文章之前已经有人做过类似的工作，他们使用了REINFORCE，将之与MLE结合创造了一种新的方法名为MIXER，然而在他们的PG方法中，每个单词返回reward和句子返回的reward相同，这通常是不对的。

在本篇文章中，作者们对前人所做的工作在以下几个方面做了拓展：
- 使用Monte Carlo rollouts对PG的实现做了改进，并使用了一个参数化的baseline估计以减少估计方差；
- 延伸原有方法对新的metric SPICE进行优化。

#### Related Work

文章中使用了"Show and Tell"(ST)模型框架。

#### Methods

>论文主题部分

##### Training using policy gradient

>很多公式推导啊，没看懂呢

##### Reward function for the policy gradient

PG方法可以用于各种回馈函数。



<!-- #### latex test -->
<!-- <img src="http://chart.googleapis.com/chart?cht=tx&chl=\Large x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}" style="border:none;"> -->